sources

ADAM - https://arxiv.org/pdf/1412.6980.pdf
more ADAM - https://towardsdatascience.com/adam-optimization-algorithm-1cdc9b12724a
Cross Entropy - https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html
Conv - https://jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/
more Conv - https://cs231n.github.io/neural-networks-1/
Other - https://web.stanford.edu/class/psych209/
RE Learn - https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf
basic q learn / bellman ford - https://my.eng.utah.edu/~cs5480/notes/chapter4-2007-Part2.pdf
problem in mind -> adaptive learning rate - https://www.ibm.com/cloud/learn/gradient-descent
quad loss function - https://www.sciencedirect.com/topics/computer-science/quadratic-loss-function
adaptive learning rate - https://www.cs.cornell.edu/courses/cs6787/2019fa/lectures/Lecture8.pdf
notation exp - https://people.duke.edu/~ccc14/sta-663-2019/notebook/S09G_Gradient_Descent_Optimization.html
**notation - https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html